{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Capstone.ipynb","provenance":[{"file_id":"1C0wYy1bbUHrJI1nvGotGgN1qfY1SgGn7","timestamp":1616311327556},{"file_id":"https://github.com/laskari/Capstone_Project_END/blob/main/Capstone_proj.ipynb","timestamp":1616093306654}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"61b60aba0421462da5ed23c0d671cff3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ed7a212823b5419c83f0e1e0c6f0f046","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_208073f2d51449e9aa7e8e6123c45d05","IPY_MODEL_ec371ff416a04ae5ae90ef36f6a8ae10"]}},"ed7a212823b5419c83f0e1e0c6f0f046":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"208073f2d51449e9aa7e8e6123c45d05":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8642002ae59e4d748c1b9645be5b9d16","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":11506,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":11506,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ec82518fdece40b1a13ff24d7e36272f"}},"ec371ff416a04ae5ae90ef36f6a8ae10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c04dad0034ad4156b993af0efd43e497","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 11506/11506 [00:00&lt;00:00, 127572.51it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e6801774b564ca89b133bb8e3aec5a8"}},"8642002ae59e4d748c1b9645be5b9d16":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ec82518fdece40b1a13ff24d7e36272f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c04dad0034ad4156b993af0efd43e497":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0e6801774b564ca89b133bb8e3aec5a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hK2ZULMsCMVE","executionInfo":{"status":"ok","timestamp":1616351972149,"user_tz":-330,"elapsed":19108,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"e879bc64-6d36-477a-e0ce-6473bd105bfe"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UqoGK7g3ckO","executionInfo":{"status":"ok","timestamp":1616351972515,"user_tz":-330,"elapsed":19465,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"697b9adf-6679-40c2-ba30-736839315dcd"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","\n","file = open('/content/drive/MyDrive/english_python_data_formatted_spaces.txt',\"rt\",encoding='latin')\n","data_complete = file.read()\n","print(data_complete[:500])\n","print(len(data_complete))\n","data_complete =  data_complete.lower()\n","\n","data_complete = re.sub(r'\"#[0123456789]','#', data_complete)\n","data_complete = data_complete.replace(\"\\n\\n\",\"\\n\")\n","data_complete = data_complete.replace(\"\\n\\n\\n\",\"\\n\\n\")\n","\n","data_com = data_complete.split('#')\n","\n","text = []\n","prog =[]\n","\n","for each_code in data_com:\n","    pr_text = each_code.split('\\n')\n","    text.append(pr_text[0])\n","    prog.append(pr_text[1:])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["# write a python program to add two numbers\n","num1 = 1.5\n","num2 = 6.3\n","sum = num1 + num2\n","print(f'Sum: {sum}')\n","\n","\n","# write a python function to add two user provided numbers and return the sum\n","def add_two_numbers(num1, num2):\n","\tsum = num1 + num2\n","\treturn sum\n","\n","\n","# write a program to find and print the largest among three numbers\n","\n","num1 = 10\n","num2 = 12\n","num3 = 14\n","if (num1 >= num2) and (num1 >= num3):\n","\tlargest = num1\n","elif (num2 >= num1) and (num2 >= num3):\n","\tlargest = num2\n","else:\n","\tlargest = num3\n","print(f'largest:{l\n","1043396\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CRQyobB24SJZ","executionInfo":{"status":"ok","timestamp":1616351972515,"user_tz":-330,"elapsed":19458,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["df = pd.DataFrame({'Text': text, 'Code': prog})"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vRK3Qcb4Yuk","executionInfo":{"status":"ok","timestamp":1616351972516,"user_tz":-330,"elapsed":19455,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"9ac6eb7b-4706-42c5-af07-17f00a89dfc2"},"source":["df.shape"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5367, 2)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"gmC9T0Ld4nqE","executionInfo":{"status":"ok","timestamp":1616351976241,"user_tz":-330,"elapsed":23174,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["import torch\n","from torch.jit import script, trace\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import csv\n","from torchtext.legacy import data\n","from torchtext.legacy.data import Field, BucketIterator\n","import random\n","import re\n","import os\n","import unicodedata\n","import codecs\n","from io import open\n","import itertools\n","import math\n","import numpy as np"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"2DkxvTE14nuS","executionInfo":{"status":"ok","timestamp":1616351976244,"user_tz":-330,"elapsed":23172,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"wm7FZCcD4ny4","executionInfo":{"status":"ok","timestamp":1616351976244,"user_tz":-330,"elapsed":23168,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["USE_CUDA = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBfSnt254n2C","executionInfo":{"status":"ok","timestamp":1616351976245,"user_tz":-330,"elapsed":23165,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"9f78e1c6-cdb9-4752-fc16-b93241da2c80"},"source":["device"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"QCPvlqd_4n49","executionInfo":{"status":"ok","timestamp":1616351980607,"user_tz":-330,"elapsed":27520,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["Code = Field(tokenize = 'spacy', \n","            init_token = 'start', \n","            eos_token = 'end', \n","            lower = True, \n","            batch_first = True)\n","\n","Text = Field(tokenize = 'spacy', \n","            init_token = 'start', \n","            eos_token = 'end', \n","            lower = True, \n","            batch_first = True)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XdOFtnO4n8w","executionInfo":{"status":"ok","timestamp":1616351980975,"user_tz":-330,"elapsed":27885,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["fields = [('Text', Text),('Code',Code)]\n","example = [data.Example.fromlist([df.Text[i],df.Code[i]], fields) for i in range(df.shape[0])] \n","dataset = data.Dataset(example, fields)\n","\n","(train_data, valid_data, test_data) = dataset.split(split_ratio=[0.80, 0.10, 0.10], random_state=random.seed(SEED))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGkvAfqaiNYL","executionInfo":{"status":"ok","timestamp":1616351980976,"user_tz":-330,"elapsed":27881,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"ec44fb7c-ea69-4f65-cdef-27ca00dae939"},"source":["print(vars(train_data.examples[2]))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["{'Text': [' ', 'write', 'a', 'program', 'that', 'adds', 'the', 'square', 'of', 'two', 'numbers', 'and', 'prints', 'it'], 'Code': ['a = 32', 'b = 21', 'result = a**2 + b**2', 'print(result)', '']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8xb_0CLp4oAV","executionInfo":{"status":"ok","timestamp":1616351980976,"user_tz":-330,"elapsed":27875,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["Text.build_vocab(train_data, min_freq = 1)\n","Code.build_vocab(train_data, min_freq = 1)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NArQAiWwE0Xh","executionInfo":{"status":"ok","timestamp":1616351980976,"user_tz":-330,"elapsed":27871,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"442ad964-8cb4-4db6-fdf5-f8a45746e7cc"},"source":["print(\"Text Vocab size\", len(Text.vocab))\n","print(\"Code Vocab size\", len(Code.vocab))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Text Vocab size 2395\n","Code Vocab size 11506\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FNSe7J4z4oDu","executionInfo":{"status":"ok","timestamp":1616351980977,"user_tz":-330,"elapsed":27866,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator = BucketIterator.splits(\n","    (train_data, valid_data), \n","     sort= False,\n","     batch_size = BATCH_SIZE,\n","     device = device)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"6BQrX3utDyp2","executionInfo":{"status":"ok","timestamp":1616351981832,"user_tz":-330,"elapsed":28716,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["import spacy\n","spacy_en = spacy.load('en')\n","def Tokenize(sentence):\n","  sentence = str(sentence).replace('\\n', '\\t\\t')\n","  return [tok.text for tok in spacy_en.tokenizer(sentence)]"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"CKxhIOP3DjLG","executionInfo":{"status":"ok","timestamp":1616351985857,"user_tz":-330,"elapsed":32734,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["import gensim\n","w2v_dim = 256\n","w2v_min_count = 2\n","w2v_window = 3\n","target = []\n","for sent in df['Code'].values:\n","  sent_token = Tokenize(sent)\n","  target.append(sent_token)\n","w2v_model = gensim.models.Word2Vec(target, size = w2v_dim, window = w2v_window, min_count = w2v_min_count)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205,"referenced_widgets":["61b60aba0421462da5ed23c0d671cff3","ed7a212823b5419c83f0e1e0c6f0f046","208073f2d51449e9aa7e8e6123c45d05","ec371ff416a04ae5ae90ef36f6a8ae10","8642002ae59e4d748c1b9645be5b9d16","ec82518fdece40b1a13ff24d7e36272f","c04dad0034ad4156b993af0efd43e497","0e6801774b564ca89b133bb8e3aec5a8"]},"id":"ZtnYrcI5FL5d","executionInfo":{"status":"ok","timestamp":1616351985859,"user_tz":-330,"elapsed":32730,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"bbf09964-9f3f-4c6a-e590-40b3ef3b2052"},"source":["from tqdm import tqdm_notebook\n","word2vec_vectors = []\n","for token, idx in tqdm_notebook(Code.vocab.stoi.items()):\n","  if token in w2v_model.wv.vocab.keys():\n","    word2vec_vectors.append(torch.FloatTensor(w2v_model[token]))\n","  else:\n","    word2vec_vectors.append(torch.zeros(w2v_dim))\n","Code.vocab.set_vectors(Code.vocab.stoi, word2vec_vectors, w2v_dim)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61b60aba0421462da5ed23c0d671cff3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=11506.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"7J-cW8reENFn","executionInfo":{"status":"ok","timestamp":1616351985859,"user_tz":-330,"elapsed":32723,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["w2v_model.save('embeddings.txt')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQzDyY4K4oHM","executionInfo":{"status":"ok","timestamp":1616351985860,"user_tz":-330,"elapsed":32721,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["class Encoder(nn.Module):\n","    def __init__(self, \n","                 input_dim, \n","                 hid_dim, \n","                 n_layers, \n","                 n_heads, \n","                 pf_dim,\n","                 dropout, \n","                 device,\n","                 max_length = 250):\n","        super().__init__()\n","\n","        self.device = device\n","        \n","        #self.tok_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(Text.vocab.vectors))\n","        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        \n","        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim,\n","                                                  dropout, \n","                                                  device) \n","                                     for _ in range(n_layers)])\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len]\n","        #src_mask = [batch size, 1, 1, src len]\n","        \n","        batch_size = src.shape[0]\n","        src_len = src.shape[1]\n","        \n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","        \n","        #pos = [batch size, src len]\n","        \n","        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        for layer in self.layers:\n","            src = layer(src, src_mask)\n","            \n","        #src = [batch size, src len, hid dim]\n","            \n","        return src"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"CpURbZPm4oLE","executionInfo":{"status":"ok","timestamp":1616351985860,"user_tz":-330,"elapsed":32716,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim,  \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len, hid dim]\n","        #src_mask = [batch size, 1, 1, src len] \n","                \n","        #self attention\n","        _src, _ = self.self_attention(src, src, src, src_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        src = self.self_attn_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        #positionwise feedforward\n","        _src = self.positionwise_feedforward(src)\n","        \n","        #dropout, residual and layer norm\n","        src = self.ff_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        return src"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5bD3V1R4oOl","executionInfo":{"status":"ok","timestamp":1616351986273,"user_tz":-330,"elapsed":33124,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout, device):\n","        super().__init__()\n","        \n","        assert hid_dim % n_heads == 0\n","        \n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","        \n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","        \n","    def forward(self, query, key, value, mask = None):\n","        \n","        batch_size = query.shape[0]\n","        \n","        #query = [batch size, query len, hid dim]\n","        #key = [batch size, key len, hid dim]\n","        #value = [batch size, value len, hid dim]\n","                \n","        Q = self.fc_q(query)\n","        K = self.fc_k(key)\n","        V = self.fc_v(value)\n","        \n","        #Q = [batch size, query len, hid dim]\n","        #K = [batch size, key len, hid dim]\n","        #V = [batch size, value len, hid dim]\n","                \n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        \n","        #Q = [batch size, n heads, query len, head dim]\n","        #K = [batch size, n heads, key len, head dim]\n","        #V = [batch size, n heads, value len, head dim]\n","                \n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","        \n","        #energy = [batch size, n heads, query len, key len]\n","        \n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","        \n","        attention = torch.softmax(energy, dim = -1)\n","                \n","        #attention = [batch size, n heads, query len, key len]\n","                \n","        x = torch.matmul(self.dropout(attention), V)\n","        \n","        #x = [batch size, n heads, query len, head dim]\n","        \n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        \n","        #x = [batch size, query len, n heads, head dim]\n","        \n","        x = x.view(batch_size, -1, self.hid_dim)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        x = self.fc_o(x)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        return x, attention"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-Z0FEnZ4oSB","executionInfo":{"status":"ok","timestamp":1616351986273,"user_tz":-330,"elapsed":33119,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","        \n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","        \n","        #x = [batch size, seq len, pf dim]\n","        \n","        x = self.fc_2(x)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        return x"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTEFKX_E4oVz","executionInfo":{"status":"ok","timestamp":1616351986274,"user_tz":-330,"elapsed":33115,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["class Decoder(nn.Module):\n","    def __init__(self, \n","                 output_dim, \n","                 hid_dim, \n","                 n_layers, \n","                 n_heads, \n","                 pf_dim, \n","                 dropout, \n","                 device,\n","                 pre_trained_emb,\n","                 max_length = 100):\n","        super().__init__()\n","        \n","        self.device = device\n","        \n","        #self.tok_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(Code.vocab.vectors))\n","        self.pre_trained_emb = pre_trained_emb\n","        self.tok_embedding = nn.Embedding.from_pretrained(self.pre_trained_emb)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        \n","        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim, \n","                                                  dropout, \n","                                                  device)\n","                                     for _ in range(n_layers)])\n","        \n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        \n","        #trg = [batch size, trg len]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        #src_mask = [batch size, 1, 1, src len]\n","                \n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        \n","        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","                            \n","        #pos = [batch size, trg len]\n","            \n","        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n","                \n","        #trg = [batch size, trg len, hid dim]\n","        \n","        for layer in self.layers:\n","            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        output = self.fc_out(trg)\n","        \n","        #output = [batch size, trg len, output dim]\n","            \n","        return output, attention"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"XcD4nRwO4oZL","executionInfo":{"status":"ok","timestamp":1616351986275,"user_tz":-330,"elapsed":33109,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim, \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        #src_mask = [batch size, 1, 1, src len]\n","        \n","        #self attention\n","        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n","            \n","        #trg = [batch size, trg len, hid dim]\n","            \n","        #encoder attention\n","        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n","        # query, key, value\n","        \n","        #dropout, residual connection and layer norm\n","        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n","                    \n","        #trg = [batch size, trg len, hid dim]\n","        \n","        #positionwise feedforward\n","        _trg = self.positionwise_feedforward(trg)\n","        \n","        #dropout, residual and layer norm\n","        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return trg, attention"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"EU8WSknE4ocy","executionInfo":{"status":"ok","timestamp":1616351986275,"user_tz":-330,"elapsed":33105,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, \n","                 encoder, \n","                 decoder, \n","                 src_pad_idx, \n","                 trg_pad_idx, \n","                 device):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","        \n","    def make_src_mask(self, src):\n","        \n","        #src = [batch size, src len]\n","        \n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        return src_mask\n","    \n","    def make_trg_mask(self, trg):\n","        \n","        #trg = [batch size, trg len]\n","        \n","        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n","        \n","        #trg_pad_mask = [batch size, 1, 1, trg len]\n","        \n","        trg_len = trg.shape[1]\n","        \n","        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n","        \n","        #trg_sub_mask = [trg len, trg len]\n","            \n","        trg_mask = trg_pad_mask & trg_sub_mask\n","        \n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        return trg_mask\n","\n","    def forward(self, src, trg):\n","        \n","        #src = [batch size, src len]\n","        #trg = [batch size, trg len]\n","                \n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        \n","        #src_mask = [batch size, 1, 1, src len]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        enc_src = self.encoder(src, src_mask)\n","        \n","        #enc_src = [batch size, src len, hid dim]\n","                \n","        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n","        \n","        #output = [batch size, trg len, output dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return output, attention"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mwb_8IqY4ogn","executionInfo":{"status":"ok","timestamp":1616351997186,"user_tz":-330,"elapsed":44011,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["INPUT_DIM = len(Text.vocab)\n","OUTPUT_DIM = len(Code.vocab)\n","HID_DIM = 256\n","ENC_LAYERS = 3\n","DEC_LAYERS = 3\n","ENC_HEADS = 8\n","DEC_HEADS = 8\n","ENC_PF_DIM = 512\n","DEC_PF_DIM = 512\n","ENC_DROPOUT = 0.1\n","DEC_DROPOUT = 0.1\n","pre_trained_emb = torch.FloatTensor(Code.vocab.vectors)\n","\n","enc = Encoder(INPUT_DIM,\n","              HID_DIM, \n","              ENC_LAYERS, \n","              ENC_HEADS, \n","              ENC_PF_DIM, \n","              ENC_DROPOUT, \n","              device)\n","\n","dec = Decoder(OUTPUT_DIM, \n","              HID_DIM, \n","              DEC_LAYERS, \n","              DEC_HEADS, \n","              DEC_PF_DIM, \n","              DEC_DROPOUT, \n","              device, \n","              pre_trained_emb)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"SI9Ydm2u4oj1","executionInfo":{"status":"ok","timestamp":1616351997189,"user_tz":-330,"elapsed":44010,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["SRC_PAD_IDX = Text.vocab.stoi[Text.pad_token]\n","TRG_PAD_IDX = Code.vocab.stoi[Code.pad_token]\n","\n","model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHTJaXIm4onU","executionInfo":{"status":"ok","timestamp":1616351997189,"user_tz":-330,"elapsed":44006,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["def initialize_weights(m):\n","    if hasattr(m, 'weight') and m.weight.dim() > 1:\n","        nn.init.xavier_uniform_(m.weight.data)\n","\n","model.apply(initialize_weights);"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"_717f0Gt4oqd","executionInfo":{"status":"ok","timestamp":1616351997190,"user_tz":-330,"elapsed":44004,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["LEARNING_RATE = 0.0005\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"F02e3HQK4ot2","executionInfo":{"status":"ok","timestamp":1616351997190,"user_tz":-330,"elapsed":44000,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"n90PVBeh4oxk","executionInfo":{"status":"ok","timestamp":1616351997191,"user_tz":-330,"elapsed":43997,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.Text\n","        trg = batch.Code\n","        \n","        optimizer.zero_grad()\n","        \n","        output, _ = model(src, trg[:,:-1])\n","                \n","        #output = [batch size, trg len - 1, output dim]\n","        #trg = [batch size, trg len]\n","            \n","        output_dim = output.shape[-1]\n","            \n","        output = output.contiguous().view(-1, output_dim)\n","        trg = trg[:,1:].contiguous().view(-1)\n","                \n","        #output = [batch size * trg len - 1, output dim]\n","        #trg = [batch size * trg len - 1]\n","            \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"3IL8dkKw4o0w","executionInfo":{"status":"ok","timestamp":1616351997191,"user_tz":-330,"elapsed":43989,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.Text\n","            trg = batch.Code\n","\n","            output, _ = model(src, trg[:,:-1])\n","            \n","            #output = [batch size, trg len - 1, output dim]\n","            #trg = [batch size, trg len]\n","            \n","            output_dim = output.shape[-1]\n","            \n","            output = output.contiguous().view(-1, output_dim)\n","            trg = trg[:,1:].contiguous().view(-1)\n","            \n","            #output = [batch size * trg len - 1, output dim]\n","            #trg = [batch size * trg len - 1]\n","            \n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNtgJ3wu7JRl","executionInfo":{"status":"ok","timestamp":1616351997192,"user_tz":-330,"elapsed":43983,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgSJXge17JWC","executionInfo":{"status":"ok","timestamp":1616352201606,"user_tz":-330,"elapsed":248390,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"ff6c4de4-5e34-44ea-d456-5e5556296a8e"},"source":["import time\n","N_EPOCHS = 50\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut6-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 0m 4s\n","\tTrain Loss: 8.050 | Train PPL: 3134.688\n","\t Val. Loss: 7.504 |  Val. PPL: 1815.982\n","Epoch: 02 | Time: 0m 3s\n","\tTrain Loss: 6.883 | Train PPL: 975.726\n","\t Val. Loss: 7.289 |  Val. PPL: 1463.480\n","Epoch: 03 | Time: 0m 3s\n","\tTrain Loss: 6.691 | Train PPL: 804.824\n","\t Val. Loss: 7.290 |  Val. PPL: 1466.095\n","Epoch: 04 | Time: 0m 3s\n","\tTrain Loss: 6.439 | Train PPL: 625.567\n","\t Val. Loss: 7.107 |  Val. PPL: 1220.039\n","Epoch: 05 | Time: 0m 3s\n","\tTrain Loss: 6.049 | Train PPL: 423.628\n","\t Val. Loss: 6.885 |  Val. PPL: 977.688\n","Epoch: 06 | Time: 0m 3s\n","\tTrain Loss: 5.639 | Train PPL: 281.205\n","\t Val. Loss: 6.651 |  Val. PPL: 773.428\n","Epoch: 07 | Time: 0m 3s\n","\tTrain Loss: 5.242 | Train PPL: 189.022\n","\t Val. Loss: 6.514 |  Val. PPL: 674.496\n","Epoch: 08 | Time: 0m 3s\n","\tTrain Loss: 4.850 | Train PPL: 127.800\n","\t Val. Loss: 6.358 |  Val. PPL: 577.106\n","Epoch: 09 | Time: 0m 3s\n","\tTrain Loss: 4.482 | Train PPL:  88.442\n","\t Val. Loss: 6.217 |  Val. PPL: 501.161\n","Epoch: 10 | Time: 0m 3s\n","\tTrain Loss: 4.142 | Train PPL:  62.899\n","\t Val. Loss: 6.099 |  Val. PPL: 445.414\n","Epoch: 11 | Time: 0m 4s\n","\tTrain Loss: 3.833 | Train PPL:  46.195\n","\t Val. Loss: 6.025 |  Val. PPL: 413.815\n","Epoch: 12 | Time: 0m 4s\n","\tTrain Loss: 3.540 | Train PPL:  34.471\n","\t Val. Loss: 5.745 |  Val. PPL: 312.551\n","Epoch: 13 | Time: 0m 3s\n","\tTrain Loss: 3.247 | Train PPL:  25.725\n","\t Val. Loss: 5.650 |  Val. PPL: 284.182\n","Epoch: 14 | Time: 0m 4s\n","\tTrain Loss: 2.987 | Train PPL:  19.832\n","\t Val. Loss: 5.508 |  Val. PPL: 246.638\n","Epoch: 15 | Time: 0m 4s\n","\tTrain Loss: 2.717 | Train PPL:  15.142\n","\t Val. Loss: 5.301 |  Val. PPL: 200.565\n","Epoch: 16 | Time: 0m 4s\n","\tTrain Loss: 2.481 | Train PPL:  11.955\n","\t Val. Loss: 5.262 |  Val. PPL: 192.790\n","Epoch: 17 | Time: 0m 4s\n","\tTrain Loss: 2.261 | Train PPL:   9.592\n","\t Val. Loss: 5.146 |  Val. PPL: 171.782\n","Epoch: 18 | Time: 0m 4s\n","\tTrain Loss: 2.050 | Train PPL:   7.767\n","\t Val. Loss: 5.103 |  Val. PPL: 164.438\n","Epoch: 19 | Time: 0m 4s\n","\tTrain Loss: 1.852 | Train PPL:   6.372\n","\t Val. Loss: 4.999 |  Val. PPL: 148.196\n","Epoch: 20 | Time: 0m 4s\n","\tTrain Loss: 1.666 | Train PPL:   5.292\n","\t Val. Loss: 4.904 |  Val. PPL: 134.798\n","Epoch: 21 | Time: 0m 4s\n","\tTrain Loss: 1.492 | Train PPL:   4.447\n","\t Val. Loss: 4.875 |  Val. PPL: 130.985\n","Epoch: 22 | Time: 0m 4s\n","\tTrain Loss: 1.322 | Train PPL:   3.751\n","\t Val. Loss: 4.815 |  Val. PPL: 123.297\n","Epoch: 23 | Time: 0m 4s\n","\tTrain Loss: 1.181 | Train PPL:   3.256\n","\t Val. Loss: 4.816 |  Val. PPL: 123.476\n","Epoch: 24 | Time: 0m 4s\n","\tTrain Loss: 1.040 | Train PPL:   2.829\n","\t Val. Loss: 4.769 |  Val. PPL: 117.772\n","Epoch: 25 | Time: 0m 4s\n","\tTrain Loss: 0.903 | Train PPL:   2.466\n","\t Val. Loss: 4.699 |  Val. PPL: 109.810\n","Epoch: 26 | Time: 0m 4s\n","\tTrain Loss: 0.790 | Train PPL:   2.203\n","\t Val. Loss: 4.714 |  Val. PPL: 111.446\n","Epoch: 27 | Time: 0m 4s\n","\tTrain Loss: 0.679 | Train PPL:   1.972\n","\t Val. Loss: 4.678 |  Val. PPL: 107.524\n","Epoch: 28 | Time: 0m 4s\n","\tTrain Loss: 0.581 | Train PPL:   1.787\n","\t Val. Loss: 4.563 |  Val. PPL:  95.857\n","Epoch: 29 | Time: 0m 4s\n","\tTrain Loss: 0.495 | Train PPL:   1.641\n","\t Val. Loss: 4.674 |  Val. PPL: 107.108\n","Epoch: 30 | Time: 0m 4s\n","\tTrain Loss: 0.432 | Train PPL:   1.540\n","\t Val. Loss: 4.518 |  Val. PPL:  91.636\n","Epoch: 31 | Time: 0m 4s\n","\tTrain Loss: 0.366 | Train PPL:   1.442\n","\t Val. Loss: 4.644 |  Val. PPL: 103.993\n","Epoch: 32 | Time: 0m 4s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 4.614 |  Val. PPL: 100.867\n","Epoch: 33 | Time: 0m 3s\n","\tTrain Loss: 0.278 | Train PPL:   1.320\n","\t Val. Loss: 4.697 |  Val. PPL: 109.645\n","Epoch: 34 | Time: 0m 4s\n","\tTrain Loss: 0.239 | Train PPL:   1.270\n","\t Val. Loss: 4.692 |  Val. PPL: 109.120\n","Epoch: 35 | Time: 0m 4s\n","\tTrain Loss: 0.219 | Train PPL:   1.244\n","\t Val. Loss: 4.601 |  Val. PPL:  99.564\n","Epoch: 36 | Time: 0m 4s\n","\tTrain Loss: 0.194 | Train PPL:   1.214\n","\t Val. Loss: 4.663 |  Val. PPL: 105.914\n","Epoch: 37 | Time: 0m 4s\n","\tTrain Loss: 0.177 | Train PPL:   1.194\n","\t Val. Loss: 4.625 |  Val. PPL: 101.975\n","Epoch: 38 | Time: 0m 4s\n","\tTrain Loss: 0.162 | Train PPL:   1.176\n","\t Val. Loss: 4.570 |  Val. PPL:  96.552\n","Epoch: 39 | Time: 0m 4s\n","\tTrain Loss: 0.148 | Train PPL:   1.160\n","\t Val. Loss: 4.723 |  Val. PPL: 112.526\n","Epoch: 40 | Time: 0m 4s\n","\tTrain Loss: 0.139 | Train PPL:   1.149\n","\t Val. Loss: 4.763 |  Val. PPL: 117.109\n","Epoch: 41 | Time: 0m 4s\n","\tTrain Loss: 0.126 | Train PPL:   1.134\n","\t Val. Loss: 4.807 |  Val. PPL: 122.353\n","Epoch: 42 | Time: 0m 4s\n","\tTrain Loss: 0.118 | Train PPL:   1.125\n","\t Val. Loss: 4.719 |  Val. PPL: 112.084\n","Epoch: 43 | Time: 0m 4s\n","\tTrain Loss: 0.108 | Train PPL:   1.114\n","\t Val. Loss: 4.662 |  Val. PPL: 105.871\n","Epoch: 44 | Time: 0m 4s\n","\tTrain Loss: 0.106 | Train PPL:   1.112\n","\t Val. Loss: 4.753 |  Val. PPL: 115.900\n","Epoch: 45 | Time: 0m 4s\n","\tTrain Loss: 0.101 | Train PPL:   1.106\n","\t Val. Loss: 4.735 |  Val. PPL: 113.914\n","Epoch: 46 | Time: 0m 4s\n","\tTrain Loss: 0.095 | Train PPL:   1.100\n","\t Val. Loss: 4.802 |  Val. PPL: 121.804\n","Epoch: 47 | Time: 0m 4s\n","\tTrain Loss: 0.089 | Train PPL:   1.093\n","\t Val. Loss: 4.877 |  Val. PPL: 131.283\n","Epoch: 48 | Time: 0m 4s\n","\tTrain Loss: 0.088 | Train PPL:   1.092\n","\t Val. Loss: 4.772 |  Val. PPL: 118.175\n","Epoch: 49 | Time: 0m 4s\n","\tTrain Loss: 0.085 | Train PPL:   1.089\n","\t Val. Loss: 4.935 |  Val. PPL: 139.066\n","Epoch: 50 | Time: 0m 4s\n","\tTrain Loss: 0.082 | Train PPL:   1.086\n","\t Val. Loss: 4.923 |  Val. PPL: 137.384\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b6oyDsDog9Dx","executionInfo":{"status":"ok","timestamp":1616352201607,"user_tz":-330,"elapsed":248371,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":["import spacy\n","def generate_code(sentence, src_field, trg_field, model, device, max_len = 50):\n","    \n","    model.eval()\n","        \n","    if isinstance(sentence, str):\n","        nlp = spacy.load('en')\n","        tokens = [token.text.lower() for token in nlp(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","        \n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n","    \n","    src_mask = model.make_src_mask(src_tensor)\n","    \n","    with torch.no_grad():\n","        enc_src = model.encoder(src_tensor, src_mask)\n","\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","\n","    for i in range(max_len):\n","\n","        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n","\n","        trg_mask = model.make_trg_mask(trg_tensor)\n","        \n","        with torch.no_grad():\n","            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n","        \n","        pred_token = output.argmax(2)[:,-1].item()\n","        \n","        trg_indexes.append(pred_token)\n","\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","            break\n","    \n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","    \n","    return trg_tokens[1:], attention"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8iubY-lFhE_5","executionInfo":{"status":"ok","timestamp":1616352202012,"user_tz":-330,"elapsed":248771,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"af8670b9-eb5a-4e1a-a32e-df246cafe9af"},"source":["Question_text = 'Write a python program to print largest of three numbers\\n '\n","\n","print(Question_text )\n","\n","translation, attention = generate_code(Question_text , Text, Code, model, device)\n","\n","for i in range(len(translation)):\n","  print(end =\"\")\n","  print(translation[i])"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Write a python program to print largest of three numbers\n"," \n","num1 = 2\n","num2 = 4\n","num1, num2 = num2, num1\n","print(num1, num2)\n","\n","end\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQh5fC6h27So","executionInfo":{"status":"ok","timestamp":1616352203065,"user_tz":-330,"elapsed":249817,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}},"outputId":"47457998-8a98-432c-9d3a-c9a87cea7fa7"},"source":["import random\n","\n","for i in range(25):\n","  print(\"++++++++++++++++++++++\\n\\n\\nExample\", i+1)\n","  inf = random.randint(1,500)\n","  src = vars(test_data.examples[inf])['Text']\n","  print(' '.join(src))\n","  translation, attention = generate_code(src , Text, Code, model, device)\n","\n","  for i in range(len(translation)):\n","    print(end =\"\")\n","    print(translation[i])"],"execution_count":37,"outputs":[{"output_type":"stream","text":["++++++++++++++++++++++\n","\n","\n","Example 1\n","  write a python function to convert byte to utf-8\n","def byte_to_utf8(data):\n","\treturn data.decode(\"utf-8\")\n","print(byte_to_utf8(data=b'r\\xc3\\xa9sum\\xc3\\xa9'))\n","\n","def hanoi(disks, source, auxiliary, target):\n","\tif disks == 1:\n","\t\tprint('move disk 1 from peg {} to peg {}.'.format(source, target))\n","\t\treturn\n","\thanoi(disks - 1, source, target, auxiliary)\n","\tprint('move disk {} from peg {} to peg {}.'.format(disks, source, target))\n","\thanoi(disks - 1, auxiliary, source, target)\n","\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 2\n","  write a python program to print even numbers in a list\n","list1  = [2,7,5,64,14]\n","for i in list1:\n","\tif i%2==0:\n","\t\tprint(i,end=\" \")\n","\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 3\n","  write a program to increment number which is at end of string\n","import re\n","str1 = 'count001'\n","res = re.sub(r'[0-9]+$',\n","\t\t\t lambda x: f\"{str(int(x.group())+1).zfill(len(x.group()))}\",\n","\t\t\t str1)\n","print(\"incremented numeric string : \" + str(res))\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 4\n","  write a function that returns log of a function\n","def log(x:float)->float:\n","\timport math\n","\treturn math.log(x)\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 5\n","15 how to add extra zeros after decimal in python\n","format(2.0, '.6f')\n","'2.000000'\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 6\n","  write a python program that would swap variable values\n","a = 10\n","b = 15\n","a, b = b, a\n","\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 7\n","  write a function to create all possible permutations from a given collection of distinct numbers\n","def permute(nums):\n","\tresult_perms = [[]]\n","\tfor n in nums:\n","\t\tnew_perms = []\n","\t\tfor perm in result_perms:\n","\t\t\tfor i in range(len(perm)+1):\n","\t\t\t\tnew_perms.append(perm[:i] + [n] + perm[i:])\n","\t\t\t\tresult_perms = new_perms\n","\treturn result_perms\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 8\n","  write a python program to read a file and capitalize the first letter of every word in the file\n","def capitalize(fname):\n","\twith open(fname, 'r') as f:\n","\t\tfor line in f:\n","\t\t\tl=line.title()\n","\t\t\tprint(l)\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 9\n","60 write a program to test for even values dictionary values lists and print it\n","import time\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 10\n","  initializing removal character\n","str1 = 'i am 25 years and 10 months old'\n","res = \"\".join([item for item in str1 if item.isdigit()])\n","print(res)\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 11\n","  write a program to replace multiple words with a single word\n","str1 = 'coffeeday is best for coffee and having long conversations'\n","word_list = [\"best\", 'long']\n","repl_word = 'good'\n","res = ' '.join([repl_word if idx in word_list else idx for idx in str1.split()])\n","print(\"string after multiple replace : \" + str(res))\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 12\n","  write a function to return the profit or loss based on cost price and selling price\n","def find_profit_or_loss(cp,sp):\n","\tif cp > sp:\n","\t\treturn 'loss', cp-sp\n","\telif cp < sp:\n","\t\treturn 'profit', sp-cp\n","\telse:\n","\t\treturn 'no profit or loss', 0\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 13\n","  write a python function to print the collatz sequence\n","def collatz(n):\n","\twhile n > 1:\n","\t\tprint(n, end=' ')\n","\t\tif (n % 2):\n","\t\t\t\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 14\n","write a function to implement stooge sort\n","def stoogesort(arr, l, h):\n","\tif l >= h:\n","\t\treturn\n","\tif arr[l] > arr[h]:\n","\t\tt = arr[l]\n","\t\tarr[l] = arr[h]\n","\t\tarr[h] = t\n","\tif h - l + 1 > 2:\n","\t\tt = (int)((h - l + 1) / 3)\n","\t\tstoogesort(arr, l, (h - t))\n","\t\tstoogesort(arr, l + t, (h))\n","\t\tstoogesort(arr, l, (h - t))\n","arr = [2, 4, 5, 3, 1]\n","n = len(arr)\n","stoogesort(arr, 0, n - 1)\n","for i in range(0, n):\n","\tprint(arr[i], end= \\' \\')\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 15\n","  write a python class to calculate area of a circle and print the vale for a radius\n","class circlearea():\n","\tdef __init__(self,radius):\n","\t\tself.radius=radius\n","\tdef area(self):\n","\t\treturn 3.14 * self.radius * self.radius\n","a=6\n","obj=circlearea(a)\n","print(\"area of rectangle:\",obj.area())\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 16\n","  write a python program to input two numbers from user and add two numbers and print the result\n","num1 = 1.5\n","num2 = 6.3\n","sum = num1 - num2\n","print(f'sub: {sum}')\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 17\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 18\n","  write a python program to remove multiple empty spaces from   list of strings . print the original and final lists .\n","test_list = ['gfg', '\t', ' ', 'is', '\t\t\t', 'best']\n","print(\"the original list is : \" + str(test_list))\n","res = [ele for ele in test_list if ele.strip()]\n","print(\"list after filtering non-empty strings : \" + str(res))\n","\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 19\n","  write a python function to check whether the number is a duck number or not\n","def fiboacci_number_check(n):\n","\tif(isinstance(n,int)):\n","\t\tresult = list(filter(lambda num : int(math.sqrt(num)) * int(math.sqrt(num)) == num, [5*n*n + 4,5*n*n - 4] ))\n","\t\treturn bool(result)\n","\telse:\n","\t\traise typeerror(\"input should be of type int\")\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 20\n","  write a python function that given five positive integers and find the minimum and maximum values that can be calculated by summing exactly four of the five integers .\n","def isvalid(s):\n","\tstack = []\n","\tmapping = {')': '(', '}' : '{', ']':'['}\n","\tfor char in s:\n","\t\tif char in mapping:\n","\t\t\tif not stack:\n","\t\t\t\treturn false\n","\t\telse:\n","\t\t\tstack.append(char)\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 21\n","  write a python function to add numbers in a list\n","def permute(nums):\n","\tresult = map(lambda x, y, z: x + y + z, num1, num2, num3)\n","\tprint(\"\\nnew list after adding above three lists:\")\n","\tprint(list(result))\n","\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 22\n","write a function to compress a given string . suppose a character ' c ' occurs consecutively x times in the string . replace these consecutive occurrences of the character ' c ' with   ( x , c ) in the string .\n","def compress(text):\n","\tfrom itertools import groupby\n","\tfor k, g in groupby(text):\n","\t\tprint(\"({}, {})\".format(len(list(g)), k), end=\" \")\n","\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 23\n","  write a python function that given five positive integers and find the minimum and maximum values that can be calculated by summing exactly four of the five integers .\n","def isvalid(s):\n","\tstack = []\n","\tmapping = {')': '(', '}' : '{', ']':'['}\n","\tfor char in s:\n","\t\tif char in mapping:\n","\t\t\tif not stack:\n","\t\t\t\treturn false\n","\t\telse:\n","\t\t\tstack.append(char)\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 24\n","  write a program to check if a string is binary or not\n","def check(string) :\n","\tp = set(string)\n","\ts = {'0', '1'}\n","\tif s == p or p == {'0'} or p == {'1'}:\n","\t\tprint(\"yes\")\n","\telse :\n","\t\tprint(\"no\")\n","\n","end\n","++++++++++++++++++++++\n","\n","\n","Example 25\n","  write a python function to strip punctuations from a given string\n","def strip_punctuations(s):\n","\treturn s.translate(str.maketrans('', '', string.punctuation))\n","\n","\n","end\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zb_7_6kSHg3W","executionInfo":{"status":"ok","timestamp":1616352203066,"user_tz":-330,"elapsed":249812,"user":{"displayName":"sunil ks","photoUrl":"","userId":"01288140856205590947"}}},"source":[""],"execution_count":37,"outputs":[]}]}